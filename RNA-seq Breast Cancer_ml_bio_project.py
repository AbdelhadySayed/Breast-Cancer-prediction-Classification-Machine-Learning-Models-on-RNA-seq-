# -*- coding: utf-8 -*-
"""MAP TEAM ML BIO Project .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13BRTOSNw59twz0Htou8HhR718JCDx893

## Data Analysis Plan

1- Import all nedeed libraries

2- Load your dataset

3- Explore your data

4- Task 1: Preprocessing, Cleaning 

5- Task 2: Classifier and Quality Measure Selection

6- Task 3: Parameter Tuning and Validation

7- Task 4: Model Training

8- Task 5: Model interpretation and feature importance

9- Task 6: External Testing Code

> ## 1- Import Nedded Libraries
"""

import pandas as pd

import numpy as np

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

"""> ## 2- Load your dataset"""

## Read the dataset as pandas dataframe
traindata_df =pd.read_csv('trainingdata.csv')

## check the head rows of your dataframe
traindata_df.head(3)

## find out the column names of your dataframe
traindata_df.columns

## Make the first column as the index column:

traindata_df = pd.read_csv('trainingdata.csv', index_col='Unnamed: 0')

## check that the first column is now the index column
traindata_df.head(3)

## Check the last set of rows in your dataframe
traindata_df.tail()

## Read the labels dataset as pandas dataframe:

target_df = pd.read_csv('traininglabels.csv')
#target_df.head(3)

## Read the labels dataset as pandas dataframe & Make the first column as the index column:

target_df = pd.read_csv('traininglabels.csv',index_col='Unnamed: 0')
target_df.columns= ['class']
#target_df

## Examine if there is null values in the target dataset:
target_df.isnull().sum()

## Find how many classes exist in the target dataset:
target_df['class'].unique()

## check how many rows & columns in your dataframes

print(target_df.shape)
print(traindata_df.shape)

"""> ## 3- Exploring your data"""

traindata_df.info()

traindata_df.describe()

"""> ## 4- Task 1: Preprocessing, Cleaning

#### 1- Check for all null columns & remove if any exists
#### 2- Check for all null rows & remove if any exists
#### 3- Check for existance of duplicate rows & remove if any exists
#### 4- Check for existance of duplicate columns & remove if any exists
#### 5- Check for existance of negative values
#### 6- Check for the existance of all zeros columns & remove if any exists
#### 7- Check for columns having nulls >= 25% of its data & remove if any exists
#### 8- Split df with respect to class and check for <u> all nan </U> or <u> all zeros</u> in each class
#### 9- Drop fraction genes
#### 10- Handling outliers 
#### 11- Data Imputation (replace missing values)
#### 12- Check correlation between columns and remove highly correlated columns 
#### 13-Dimensionality reduction
#### 14- Feature scaling

> ## Preprocessing steps: 
> #### 1- Check for all null columns & remove if any exists
"""

## Calculate total number of columns that had null values 
Null_columns =[]
for col in traindata_df.columns:
    n = traindata_df[col].isnull().any()
    if n == True:
        Null_columns.append(col)
print (len(Null_columns))

## Find if there were genes (columns) having all it's elements as null values, and counting them:

AllNull_genes = []
for column in traindata_df.columns:
    All_null = traindata_df[column].isnull().sum()
    if All_null == 600:         ## This means that this gene has 600 null entries corresponding to the 600 samples
        #print (column)
        AllNull_genes.append(column)
print(AllNull_genes)        
print (len(AllNull_genes))

## Drop genes (columns) with all null (missing) elements
No_AllNulls_df = traindata_df.dropna(axis=1, how='all')

"""### >>> 50 columns were found having all their elements as 'Nan' and were removed"""

No_AllNulls_df.shape

"""> ## Preprocessing steps: 
> #### 2- Check for all null rows & remove if any exists
"""

# find out the number of null values in each row

No_AllNulls_df.isnull().sum(axis=1)

## Find if there were samples (rows) having all it's elements as null values and counting them:


Samples = dict(No_AllNulls_df.isnull().sum(axis=1))
Null_samples = []
for key, value in Samples.items():
    #print(key)
    if value == 4950:
        Null_samples.append(key)
print(Null_samples)
print(len(Null_samples))

"""### >>>No rows having all its values as null

> ## Preprocessing steps: 
> #### 3- Check for existance of duplicate rows & remove if any exists
"""

## To check if there is any duplicate row:

No_AllNulls_df.drop_duplicates()

"""### >>> No duplicate rows found

> ## Preprocessing steps: 
> #### 4- Check for existance of duplicate columns & remove if any exists
"""

NoDuplicate_col_df = No_AllNulls_df.T.drop_duplicates().T
NoDuplicate_col_df.shape

"""### >>>  72 duplicate columns were found and removed

> ## Preprocessing steps: 
> #### 5- check for the existance of negative values
"""

## To check if there is any faulty entry that has negative values

(NoDuplicate_col_df<0).any().any()

"""### >>> No negative values found

> ## Preprocessing steps: 
> #### 6- Check for the existance of all zeros columns & remove if any exists
"""

# Searching for genes withh all zeros elements
# Find range of each gene to detect if there are zero ranges which means genes having all its values as ZERO


zero_genes = []      ## Genes all its values are zeros i.e. these genes are not expressed in any of the classes 
for col in NoDuplicate_col_df.columns:
    col_max = max(NoDuplicate_col_df[col])
    col_min = min (NoDuplicate_col_df[col])
    col_range = col_max - col_min
    if col_range == 0:
        zero_genes.append(col)
print(zero_genes)
print(len(zero_genes))

## Remove genes that has all its values as 'Zeros':

No_AllZeros_df = NoDuplicate_col_df.drop(columns=zero_genes)

## Double check that the above genes really had all elements as zeros:
NoDuplicate_col_df['gene_17'].value_counts()

"""### >>> 2 columns were found having all their elements as zeros and were removed"""

No_AllZeros_df.shape

"""> ## Preprocessing steps:
> #### 7- Check for columns having nulls >= 25% of its data & remove if any exists
> ##### N.B. This step is highly subjective and no concrete universal guidelines were found for this. 
"""

print('max.no. of null values in genes is= ',max(No_AllZeros_df.isnull().sum()), 'out of 600')
print('max. percentage of null values in genes is= ',max(No_AllZeros_df.isnull().sum())/600*100 ,'%')

## Genes having null values >= 25%:
## Making a list with these gene names and counting them.

genes_with_25Per_nulls = []
for column in No_AllZeros_df.columns:
    with_null = No_AllZeros_df[column].isnull().sum()
    if with_null >= 150:         ## This means that this gene has a number of null values >= 75 %
        #print (column)
        genes_with_25Per_nulls.append(column)
#print(genes_with_25Per_nulls)        
print (len(genes_with_25Per_nulls))

NoMore_than25per_NA_df= No_AllZeros_df.drop(columns=genes_with_25Per_nulls,axis=1)

NoMore_than25per_NA_df.shape

## Finding the minimum and maximum number of null values among different columns (genes)
 
print(min(NoMore_than25per_NA_df.isnull().sum()))
print(max(NoMore_than25per_NA_df.isnull().sum()))

"""## At this point we removed a total of:
#### 50(all null col) + 72(duplicate col) + 2(all zeros col) + 350 (>=25% of gene elements null) = 474 columns out of 5000 columns 
#### so our dataframe now had 600 rows and 4526 columns

#####  Since there still is/are column(s) with a maximum of 131 out of 600 null values we want to check if all of these null values are associated with a specific class so we will divide our 'NoMore_than25per_NA_df' dataframe into two, 'df0' for those with '0' class and the 'df1' for those with '1' class to check them separately for the existance of  genes with all Nan values, and if any exists the whole gene will be removed from the'NoMore_than25per_NA_df' dataframe, because this will affect the training process and cauze bias

> ## Preprocessing steps:
> #### 8- Split df with respect to class and check for <u> all nan </U> or <u> all zeros</u> in each class
"""

# combine the target_df to the NoMore_than75per_NA_df:

combined_df = pd.concat([NoMore_than25per_NA_df,target_df],axis=1)
combined_df.shape

## Split the dataset into two one for those with '0' class and one for those with '1' class

df0 = combined_df[combined_df['class'] == 0]
df1 = combined_df[combined_df['class'] == 1]

print (df0.shape)
print (df1.shape)

## Find if there were genes (columns) having all it's elements as null values in the df0 dataframe:

df0_Null_genes = []
for column in df0.columns:
    All_null = df0[column].isnull().sum()
    if All_null == 375:  ## if True this means that this gene has 375 null entries corresponding to the 375 samples
        #print (column)
        df0_Null_genes.append(column)
print(df0_Null_genes)        
print (len(df0_Null_genes))

## Double check to see that the maximum number of nulls in any of the columns in df0 is less than 375:

print(min(df0.isnull().sum()))
print(max(df0.isnull().sum()))

## Find if there were genes (columns) having all it's elements as null values in the df1 dataframe:

df1_Null_genes = []
for column in df1.columns:
    All_null = df1[column].isnull().sum()
    if All_null == 225:  ## if True this means that this gene has 225 null entries corresponding to the 225 samples
        #print (column)
        df1_Null_genes.append(column)
print(df1_Null_genes)        
print (len(df1_Null_genes))

## Double check to see that the maximum number of nulls in any of the columns in df1 is less than 225:

print(min(df1.isnull().sum()))
print(max(df1.isnull().sum()))

"""### Conclusion:  No all Nan columns found in any of the dataframes 'df0' nor 'df1' """

## Making copies of the two dataframes df0 & df1 but without the 'class' column:
df0_copy = df0.iloc[:, :-1]
df1_copy = df1.iloc[:, :-1]

# Searching for genes withh all zeros elements in df0_copy
# Find range of each gene to detect if there are zero ranges which means genes having all its values as ZERO in df0_copy

zero_gen = []      ## Genes all its values are zeros i.e. these genes are not expressed in class '0' 
for col in df0_copy.columns:
    col_max = max(df0_copy[col])
    col_min = min (df0_copy[col])
    col_range = col_max - col_min
    if col_range == 0:
        zero_gen.append(col)
print(zero_gen)
print(len(zero_gen))

# Searching for genes with all zeros elements in df1_copy
# Find range of each gene to detect if there are zero ranges which means genes having all its values as ZERO in df1_copy


zero_gen = []      ## Genes all its values are zeros i.e. these genes are not expressed in class 1 
for col in df1_copy.columns:
    col_max = max(df1_copy[col])
    col_min = min (df1_copy[col])
    col_range = col_max - col_min
    if col_range == 0:
        zero_gen.append(col)
print(zero_gen)
print(len(zero_gen))

## Check the unique values in these four genes in df1_copy
for gene in zero_gen:
    print(gene + ' has unuque values and their counts  = ' + str(df1_copy[gene].value_counts()))
    print(gene + 'has null values = ' + str(df1_copy[gene].isnull().sum()))

## Check the unique values in these four genes in df0_copy
for gene in zero_gen:
    print(gene + ' has unuque values and their counts  = ' + str(df0_copy[gene].value_counts()))
    print(gene + 'has null values = ' + str(df0_copy[gene].isnull().sum()))

"""# N.B.: 
['gene_342', 'gene_2826', 'gene_4399', 'gene_4863']

#### After examining these 4 genes in both data sets, we found that almost all of their elements contain null and zeros and must be removed
"""

## Drop these 4 genes from each class
df0_copy = df0_copy.drop(columns=zero_gen)
df1_copy = df1_copy.drop(columns=zero_gen)
print (df0.shape)
print (df1.shape)
print (df0_copy.shape)
print (df1_copy.shape)

"""> ## Preprocessing steps:
>#### 9- Drop All genes that has the maximum reads of 1 
"""

combined_df = pd.concat([df0_copy,df1_copy])
combined_df.shape

# Here we want to find how many elements out of the 600 elements 
## in each single gene had expression values less than one: 

low_genes =[]   ### create an empty list to write gene names 
for col in combined_df.columns:   ### loop over each gene (column)
   if max(combined_df[col])<1:
        low_genes.append(col)
print(len(low_genes))

## create new dataframe 'clean_df1' with these fraction genes dropped from the combined_df:

clean_df1 = combined_df.drop(low_genes, axis=1)
clean_df1.shape

"""## At this point we removed a total of: 
###### 50(all null col) + 72(duplicate col) + 2(all zeros col) + 350 (>=25% of gene elements null) + 4 (mixed null& zeros) + 571 (low count genes)= 1049 genes out of 5000 genes

#### so our dataframe now had 600 rows and 3951 columns

> ## Preprocessing steps: 
> #### 10- Handling outliers

> ##### a- Detect genes having extreme outlier values (>3*IQR) in each row and remove common outlier genes among all rows
> #### b- Remove outlier samples using binning and distribution of sum of read counts throughout the rows
"""

outlier_per_row = {}  ### create empty dictionary to write 600 {key:value} = {sampl_name : [list of outlier genes]}
outlier_per_gene = {}
for row in clean_df1.index:  ### loop over every row in our dataframe
    outlier_counter = 0      ### create a counter variable to count no. of genes having outlier values in each row
    outlier_genes = []       ### create an empty list to write name of genes having outlier values in each row
    outs = []
    Q1 = clean_df1.loc[row].quantile(0.25)   ### find first quartile for each row
    Q3 = clean_df1.loc[row].quantile(0.75)   ### find third quartile for each row
    IQR = Q3 - Q1                            ### find IQR for each row 
    lower_lim = Q1-3*IQR                   ### find upper limit for each row 
    upper_lim = Q3+3*IQR                   ### find lower limit for each row 
    for i, v in enumerate(clean_df1.loc[row]):  ### loop over every element in the row 
        if v>upper_lim:   ### if the value of this element is >upper_lim 
            outlier_counter += 1          ### add the counter by 1 to count no. of genes having outlier values
            outlier_genes.append(clean_df1.columns[i]) ### append gene name having this outlier value to [outlier_genes] list
            #print('row:', row[16:],'outlier:', v, 'upper lim:', upper_lim) ### print row name, outlier, upper limit
    #print('  ---------------   ')
    print(row, 'all outliers:', outlier_counter)  
    outlier_per_row[row] = outlier_genes   ### add {sampl_names : [list of outlier genes]} to the created dictionary
    print('**********************')

# get the common outlier genes throughout all rows

comm_out_genes = list(set.intersection(*map(set, outlier_per_row.values())))
print(len(comm_out_genes))
print(comm_out_genes)

# check min, max values in those outlier genes to assure that they are already outliers
# and not effective for classification 

for gene in comm_out_genes:
  print(gene, min(clean_df1[gene]), max(clean_df1[gene]))

## Drop all this list of common outlier genes from our dataframe

clean_df2 = clean_df1.drop(comm_out_genes, axis=1)
clean_df2.shape

""":b- Remove outlier samples using binning and distribution of sum of read counts throughout the rows"""

# histogram of 100 bins of row sums and distribution counts

df_sum = clean_df2.sum(axis=1)
plt.hist(df_sum, bins=100)

# from the above histogram we decide to remove rows with sum <34200 or >36000 and see the shape of dataframe

print(clean_df2.shape)
clean_df3 = clean_df2.loc[df_sum>34200]
print(clean_df3.shape)
clean_df3 = clean_df3.loc[df_sum<36000]
print(clean_df3.shape)

"""**Finally check that classes ratio not affected by removal of outlier rows**"""

# combine clean_df3 with target after removal of outlier rows from target
clean_target = target_df.loc[clean_df3.index]
clean_df3 = pd.concat([clean_df3,clean_target], axis=1)
clean_df3.shape

# check the shape of each class number of rows

clean_df3_0 = clean_df3[clean_df3['class'] == 0]
clean_df3_1 = clean_df3[clean_df3['class'] == 1]
print(clean_df3_0.shape)
print(clean_df3_1.shape)

"""**note:** ratio of each class not affected by removal of outlier rows

## At this point we removed a total of: 
###### 50(all null col) + 72(duplicate col) + 2(all zeros col) + 350 (>=25% of gene elements null) + 4 (mixed null& zeros) + 571 (low count genes) + 380 (extreme outliers)= 1429 genes out of 5000 genes
###### 29 outlier samples from 600 = 571

#### so our dataframe now had 600 rows and 3571 columns and 571 rows

> ## Preprocessing steps:
>#### 11- Replacing missing values
"""

# replace null values in class 0 by mean of each column

for column in clean_df3_0.columns:
    col_mean = clean_df3_0[column].mean()
    clean_df3_0[column].fillna(col_mean,inplace= True)

# replace null values in class 1 by mean of each column

for column in clean_df3_1.columns:
    col_mean = clean_df3_1[column].mean()
    clean_df3_1[column].fillna(col_mean,inplace= True)

# check that all null values were replaced 

print (clean_df3_0.isnull().sum().sum())
print (clean_df3_1.isnull().sum().sum())

# finally combine the two classes dataframes

clean_df3 = pd.concat([clean_df3_0,clean_df3_1],axis=0)
clean_df3.shape

"""> ## Preprocessing steps:
> #### 12- Check correlation between columns and remove highly correlated columns 
"""

X = clean_df3.iloc[:,:-1]
y = clean_df3.iloc[:, -1]

# with the following function we can select highly correlated features
# it will remove the first feature that is correlated with anything other feature

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

# check correlated genes with threshold corr >= 0.7
corr_features = correlation(X, 0.7)
len(set(corr_features))

corr_features

# drop all correlated genes

X = X.drop(corr_features,axis=1)
print(X.shape)

"""## At this point we removed a total of: 
###### 50(all null col) + 72(duplicate col) + 2(all zeros col) + 350 (>=25% of gene elements null) + 4 (mixed null& zeros) + 571 (low count genes) + 380 (extreme outliers) + 37 (correlated genes) = 1466 genes out of 5000 genes
###### 29 outlier samples from 600 = 571

#### so our dataframe now had 600 rows and 3534 columns and 571 rows

> ## Preprocessing steps:
> #### 12- Feature scaling 

We will use feature scalling by log2 transformation
"""

#log transformation
for col in X.columns:
    X[col] = X[col].apply(lambda x: np.log2(x + 1))

"""> ## 5- Task 2: Classifier and Quality Measure Selection

a) Select at least two classifiers to solve the classification task at hand. Motivate the choice of your classifiers (e.g. complexity, performance, interpretability). 


b) Select at least two quality measures to evaluate model performance and justify your choice (Lectures 6 and 8).

**Steps :**


**1.**   **We chose three classifier models: k neighbors, decision tree and support vector machines as our train data are small ensembles models are not suitable to avoid overfit**

**2.**   **Apply fitting of the three models using 3 Kfold cross validation to use more data in training as our data is small**

**3.**   **Use 5 evaluation quality metrics for comparision between the three models
and choose the best two of them**
"""

from sklearn.neighbors import KNeighborsClassifier 
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn import model_selection
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, accuracy_score, roc_auc_score, average_precision_score, matthews_corrcoef

# three model variables with middle parameters as possible
clf_A = KNeighborsClassifier(n_neighbors=10)
clf_B = DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=1, random_state=42)
clf_C = SVC()

# use of 5 quality metrics to be used in k fold cross validation
scoring = {'acc' : make_scorer(accuracy_score),
           'f1' : make_scorer(f1_score), 
           'roc_auc' : make_scorer(roc_auc_score),
           'PR_auc' : make_scorer(average_precision_score),
           'matcf': make_scorer(matthews_corrcoef)}

# loop through the three models and fit all X, y and evaluate the model using 5 kfold cross validation
# and store each type of score range in a list then store total scores for each model in a dictionary

results = {}
for clf in [clf_A, clf_B, clf_C]:
  kfold = StratifiedKFold(n_splits = 3, shuffle=True)
  score = model_selection.cross_validate(clf, X, y, cv = kfold, scoring = scoring)
  clf_name = clf.__class__.__name__
  results[clf_name] = score

# see the score results for the three models
results

# loop through result dictionary and store the mean for each score for all three models
for model, test in results.items():
  for (k1,v1)in test.items():
    if 'time' in k1:
      test[k1] = round(v1.max(), 2)
    else:
      test[k1] = round(v1.mean(), 2)

results

"""**From the above quality evaluation scores, we will chose decision tree and svm classifiers. we depended on percision recall score in our evaluation as our data is not balanced ~ 62% for class 0 and ~ 38% for class 1**

> ## 6- Task 3: Parameter Tuning and Validation

a) Carefully choose the set of parameters of the selected classifiers.

b) Decide for a validation strategy (e.g. Hold-out, Cross Validation, Monte Carlo Validation, see Lecture 8), carefully select the ratios between training and validation, number of folds, or percentage of data  (e.g. 7:3, 8:2, 9:1) and justify your decision.

c) Use the validation strategy to evaluate the chosen range of parameters. Create a plot depicting the relation between parameter range and quality measures. Use the plot to find the optimal set of parameters for each method. Discuss what the best parameters are and how these were chosen.

d) Plot a ROC curve for the best performing models

**Steps :**


**1.**  **First we can't use hold out strategy as our data is small so it is not good to split it. secondly, Using Kfold CV only will end with high varaince and using MCCV only is not good for this small data as it leads to high bias.
Finally, to avoid high bias and high variance we used Kfold with MCCV we made this combination by using RepeatedStratifiedKFold method from sklearn that repeats the splits randomly in each iteration (so, we used 10 loops of 3 Kfold splits with total 30 splits)** 

**2.**   **Apply hyperparameter tunning for decision tree and svm models using three scores of accuracy, PR AUC and ROC AUC.**

**3.**   **Plotting of each parameters combination with the mean score from cross validation of 3 scores; accuracy, PR AUC and ROC AUC for each model to get the best parameters and at the end plot ROC curve for each model with best parameters**

**4.**   **Compare the variance of the best parameters combinations for each model using 30 score for each split** 

**5.**   **Select the model with high score in best parameters, high roc curve AUC and low variance**
"""

from sklearn.model_selection import RepeatedStratifiedKFold

# decision tree hyper parameters
max_depth= [2, 3, 5, 10, 20]
min_samples_leaf= [2, 5, 10, 20]
criterion = ["gini", "entropy"]

# scoring metrics
scoring = {'acc' : make_scorer(accuracy_score),
           'PR_auc' : make_scorer(average_precision_score),
           'roc_auc' : make_scorer(roc_auc_score)}

# use 10 loops of 3 kfolds and make randomization in each reptition
cv= RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=1)

# dt parameter hypertunning and using cross validation
dt_scores = {}
for d in max_depth:
  for s in min_samples_leaf:
    for c in criterion:
      model= DecisionTreeClassifier(criterion=c, max_depth=d, min_samples_leaf=s, random_state=42)
      score = model_selection.cross_validate(model, X, y, cv = cv, scoring = scoring)
      dt_scores[str(d) + '/'+ str(s)+'/' + str(c)]=score
dt_scores

# loop through result dictionary and store the mean for each score and store the parameter range

dt_param = []
prauc_dt =[]
acc_dt =[]
rocauc_dt =[]
total_dt= {}
for p, s in dt_scores.items():
  scores=[]
  for (k, v)in s.items():
    if 'time' in k:
      continue
    elif 'PR_auc'in k:
      prauc_dt.append(round(v.mean(), 2))
      scores.append(v)
    elif 'roc_auc'in k:
      rocauc_dt.append(round(v.mean(), 2))
    elif 'acc'in k:
      acc_dt.append(round(v.mean(), 2))
      dt_param.append(p)
  total_dt[p]=scores

#print(total_dt)
print(len(dt_param))
print(len(prauc_dt))
print(len(acc_dt))
print(len(rocauc_dt))

# plot the parameter range with 3 types of scores of decision tree model

fig = plt.figure(figsize =(14, 7))
fig.suptitle('Hyperparameter tuning DT')
ax = fig.add_subplot()
bp = plt.plot(dt_param, acc_dt, color='red', label='Max Acc: '+str(max(acc_dt)), lw=1)
bp = plt.plot(dt_param, rocauc_dt, color='green', label='Max ROC auc: '+str(max(rocauc_dt)), lw=1)
bp = plt.plot(dt_param, prauc_dt, color='blue', label='Max PR auc: '+str(max(prauc_dt)), lw=1)
ax.set_xticklabels(dt_param)
plt.xticks(fontsize=12, rotation=90)
plt.yticks(fontsize=12)
plt.legend(loc=4, prop={'size': 10})
plt.show()

from sklearn.model_selection import RepeatedStratifiedKFold

# paramters of svm models to be tuned
C= [0.1, 1, 3]
gamma= ['scale', 'auto']
kernel= ['rbf', 'linear', 'poly']

# score metrics
scoring = {'acc' : make_scorer(accuracy_score),
           'PR_auc' : make_scorer(average_precision_score),
           'roc_auc' : make_scorer(roc_auc_score)}

# use 10 loops of 3 kfolds and make randomization in each reptition
cv= RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=1)

# svm hyperparameter tunning using cross validation
svm_scores = {}
for c in C:
  for g in gamma:
    for k in kernel:
      model= SVC(C=c, gamma=g, kernel=k)
      score = model_selection.cross_validate(model, X, y, cv = cv, scoring = scoring)
      svm_scores[str(k) + '/'+ str(c)+'/' + str(g)]=score
svm_scores

# loop through result dictionary and store the mean for each score and parameter range 

svm_param = []
prauc_svm =[]
acc_svm =[]
rocauc_svm =[]
total_svm= {}
for p, s in svm_scores.items():
  scores=[]
  for (k, v)in s.items():
    if 'time' in k:
      continue
    elif 'PR_auc'in k:
      prauc_svm.append(round(v.mean(), 2))
      scores.append(v)
    elif 'roc_auc'in k:
      rocauc_svm.append(round(v.mean(), 2))
    elif 'acc'in k:
      acc_svm.append(round(v.mean(), 2))
      svm_param.append(p)
  total_svm[p]=scores

#print(total_svm)
print(len(svm_param))
print(len(prauc_svm))
print(len(acc_svm))
print(len(rocauc_svm))

# plot the parameter range with 3 types of scores of decision tree model

fig = plt.figure(figsize =(12, 7))
fig.suptitle('Hyperparameter tuning SVM')
ax = fig.add_subplot()
bp = plt.plot(svm_param, acc_svm, color='red', label='Max Accuracy: '+str(max(acc_svm)), lw=1)
bp = plt.plot(svm_param, rocauc_svm, color='green', label='Max ROC auc: '+str(max(acc_svm)), lw=1)
bp = plt.plot(svm_param, prauc_svm, color='blue', label='Max PR auc: '+str(max(prauc_svm)), lw=1)
ax.set_xticklabels(svm_param)
plt.xticks(fontsize=12, rotation=90)
plt.yticks(fontsize=12)
plt.legend(loc=4, prop={'size': 12})
plt.show()

from sklearn.metrics import roc_curve

# split the data into train and test 
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

# SVM and DT models with the best parameters
SVM = SVC(C= 0.1, gamma='scale', kernel= 'linear', probability=True)
DT= DecisionTreeClassifier(criterion= 'entropy', max_depth= 5, min_samples_leaf=5, random_state=42)

# model fit on train data
SVM.fit(x_train, y_train)
DT.fit(x_train, y_train)

# prediction of the model
pred_svm = SVM.predict(x_test)
pred_dt = DT.predict(x_test)

# calculate roc auc for each model
AUC0 = roc_auc_score(y_test, pred_svm)
AUC1 = roc_auc_score(y_test, pred_dt)

# get false and true predictions for each model
fpr0, tpr0, thresholds0 = roc_curve(y_test, pred_svm)
fpr1, tpr1, thresholds1 = roc_curve(y_test, pred_dt)

# plot roc auc with calculated AUC values
plt.figure(figsize=(9, 6))
plt.plot(fpr0,tpr0, color='blue', label="SVM AUC="+str(round(AUC0, 2)), lw=1)
plt.plot(fpr1,tpr1, color='green', label="DT AUC="+str(round(AUC1, 2)), lw=1)

plt.legend(loc=4, prop={'size': 12})

plt.show()

# print mean and variance of pr auc score for decision tree model best parameters
for p, s in total_dt.items():
  if p == '5/5/entropy':
    print('DT: ', p, ': Mean(PR_AUC): ', round(np.mean(s),2), ', Var(PR_AUC): ', round(np.var(s),4))
    print('=================================')

# print mean and variance of pr auc score for svm model best parameters
for p, s in total_svm.items():
  if p == 'linear/0.1/scale':
    print('SVM:', p, ': Mean( PR_AUC): ', round(np.mean(s),2), ', Var(PR_AUC): ', round(np.var(s),4))

"""**Final Evaluation:**

**1- SVM model is better than DT model in PR AUC score after tunning and in ROC curve auc score, and also SVM has low variance than DT.**

**2- linear and poly kernels are the same in performance. C=0.1, gamma = scale are the best parameters in SVM model. So, we will choose linear kernel for simplicity with the best parameters**

> ## 7- Task 4: Model Training

a) Select one classifier/ model that is most generalizable and will perform best on the external test set.

b) With chosen classifiers and optimal parameters, train your model with data (training data).
"""

# initiation of the linear svm model with the best parameters

SVM_clf = SVC(C=0.1, kernel='linear', gamma='scale', probability=True)

# fit the model on the training data
SVM_clf.fit(X, y)

"""> ## 8- Task 5: Model interpretation and feature importance

a) Depending on your choice of model, select an adequate feature importance method (e.g. Linear weights, Gini Index, forward/backward selection, see Lecture 9) and explain your decision.

b) Identify the 10 most informative features for your model and visualize them in a bar plot sorted by their importance.

**Steps:**

**1- Use linear weights calculated by the linear model of svm to see the feature importance and plot the top 10 features.**

**2- Use pearson correlation as another way to see the feature importance and plot the top 10 features with weights**
"""

import seaborn as sns

# train features
features = X.columns

# weights of the features from model coeficients attribute
coef = SVM_clf.coef_[0]

#map each feature with its importance score
f_coef = []
for f, c in zip(features, coef):
  f_coef.append([f, c])

# sort the list of feature weights and get the higher 10 features
top = sorted(f_coef, key = lambda x : x[1])[-10:]

# loop through top list and store the feature and weights in lists
top_col = []
top_cor = []
for i in top:
  top_col.append(i[1])
  top_cor.append(i[0])

# plot top 10 features with the weights
g = sns.barplot(top_cor, top_col)
g.set_xticklabels(g.get_xticklabels(), rotation=90)

print(top)

from scipy import stats

# store the correlation coeficients with each feature in a list
pvs = []
for i in X.columns:
  r = stats.pearsonr(X[i], y)
  pvs.append([i, r[0]])

# sort the list according to weights and get the higher 10 
top = sorted(pvs, key = lambda x : x[1])[-10:]

# store the name of column and coeficients in lists
top_col = []
top_cor = []
top = sorted(pvs, key = lambda x : x[1])[-10:]
for i in top:
  top_col.append(i[1])
  top_cor.append(i[0])

# plot the columns with their importance
g = sns.barplot(top_cor, top_col)
g.set_xticklabels(g.get_xticklabels(), rotation=90)

print(top)

"""> ## 9- Task 6: External Testing Code

a) Load external test dataset stored in the following two files: “testdata.csv” and “testlabels.csv”. Both will have the same format as the given dataset. At the time of testing, we will copy these into your collaborative file and run your test code.

b) Prepare code for your classifier to be applied to the loaded data and labels. Note: If you selected a set of features for your finale model, please make sure that you also reduce the test dataset to those features.

c) Store the predicted labels in a vector named “finaltest_pred” (see Exercise week 10).

d) Store the predicted probabilities in a vector named “finaltest_prob” (see Exercise week 10).

**Steps:**


**1.**   **load the test data and labels and convert them to dataframes**

**2**.   **Reduce columns of testdata with the same columns present in traindata**

**3.**   **Impute null values in testdata with the train column mean for each class (the same value used for imputing in traing data)**

**4.**   **log transformation for testdata using log2**

**5.**   **store final test data as finaltest**

**6.   make prediction of the model on finaltest and store the predicted labels as finaltest_pred and predicted probabilites as finatest_prob**
"""

# convert testdata to dataframe and make Unnamed:0 column as index

testdata =pd.read_csv('testdata.csv', index_col='Unnamed: 0')

# convert testlabels to dataframe and make Unnamed:0 column as index

test_label = pd.read_csv('testlabels.csv', index_col='Unnamed: 0')

# rename the label column to class 

test_label.columns = ['class']

# discard the features that were discarded from train data 

test2= testdata.filter(X.columns)
#test2.shape

# add the testlabel to the testdata dataframe

lab_test = pd.concat([test2,test_label],axis=1)

# split the testdata into class0 and class1 dataframes

test0=lab_test[lab_test['class']==0]
test1=lab_test[lab_test['class']==1]

# impute the null value in the testdata columns by the same value we used for imputing
# in the train data mean(train(col) of class 0

for col in test0.columns:
    train0_col_mean = clean_df3_0[col].mean()
    test0[col].fillna(train0_col_mean, inplace= True)

# impute the null value in the testdata columns by the same value we used for imputing
# in the train data mean(train(col) of class 1

for col in test1.columns:
    train1_col_mean = clean_df3_1[col].mean()
    test1[col].fillna(train1_col_mean, inplace= True)

# combine the two class dataframes to each other after imputing of null values

comb_test = pd.concat([test0,test1],axis=0)

# store testing data without labels in finaltest dataframe

final_test = comb_test.iloc[:, :-1]

# log transformation of finaltest data

for col in final_test.columns:
 final_test[col] = final_test[col].apply(lambda x: np.log2(x + 1))

# Store the predicted labels as finaltest_pred

finaltest_pred = SVM_clf.predict(final_test)

# Store the predicted probabilities in finaltest_prob

finaltest_prob = SVM_clf.predict_proba(final_test)[:,1]

"""## Submission
Please submit your final collaborative file latest 12th of September to anne-christin.hauschild@med.uni-goettingen.de

"""